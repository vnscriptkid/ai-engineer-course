{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59c8772-6c17-46b2-8315-ddc95f761e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanhnguyen/anaconda3/envs/rag-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported! You're good to go!\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries for file handling and text processing\n",
    "import os, pathlib, textwrap, glob\n",
    "\n",
    "# Load documents from various sources (URLs, text files, PDFs)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Split long texts into smaller, manageable chunks for embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store to store and retrieve embeddings efficiently using FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate text embeddings using OpenAI or Hugging Face models\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "# Use local LLMs (e.g., via Ollama) for response generation\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Build a retrieval chain that combines a retriever, a prompt, and an LLM\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create prompts for the RAG system\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported! You're good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fbd9881-941c-4198-8b1d-64ad4dfdf654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 PDF pages from 4 files.\n",
      "Fetched 2 documents from the web.\n",
      "Loaded 10 documents total (pdf + web).\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader\n",
    "\n",
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "raw_docs = []\n",
    "\n",
    "# --- Load PDFs (each page is a Document) ---\n",
    "for path in pdf_paths:\n",
    "    raw_docs.extend(PyPDFLoader(path).load())\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")\n",
    "\n",
    "URLS = [\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/shipping\",\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    loader = UnstructuredURLLoader(urls=URLS)\n",
    "    web_raw_docs = loader.load()\n",
    "    print(f\"Fetched {len(web_raw_docs)} documents from the web.\")\n",
    "\n",
    "    # ‚úÖ Correct: extend with a list of Documents\n",
    "    raw_docs.extend(web_raw_docs)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  Web fetch failed, using offline copies:\", e)\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} documents total (pdf + web).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d97ffa39-5195-4ac8-a0a9-b2a67eb25db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 113 chunks ready for embedding\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "print(f\"‚úÖ {len(chunks)} chunks ready for embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b817572-c1e8-4181-a352-d33aca1b7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store with 113 embeddings\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"thenlper/gte-small\"\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Vector store with\", vectordb.index.ntotal, \"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46fc7c9-cd8b-4e43-9347-71b34f082068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9r/w28bmgxd1tlg0shgx71kk7vh0000gp/T/ipykernel_17946/2986330770.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) is a technique that combines a large language model with an external knowledge base to improve its responses by retrieving relevant information before generating a response.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "print(llm.invoke(\"Explain RAG in one sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7356292-d88e-4fa6-a538-eef29a1a1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# SYSTEM_TEMPLATE = \"\"\"\n",
    "# You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "# If the answer is not in CONTEXT, respond with ‚ÄúI'm not sure from the docs.‚Äù\n",
    "\n",
    "# Rules:\n",
    "# 1) Use ONLY the provided <context> to answer.\n",
    "# 2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "# 3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "# 4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "# CONTEXT:\n",
    "# {context}\n",
    "\n",
    "# USER:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a Customer Support Chatbot.\n",
    "\n",
    "You MUST answer using ONLY the information in <context>.\n",
    "If the answer is not in <context>, reply exactly: \"I'm not sure from the docs.\"\n",
    "\n",
    "Rules:\n",
    "1) If the answer IS in <context>, quote the exact sentence(s) that support it.\n",
    "2) If the answer is NOT in <context>, reply exactly: \"I'm not sure from the docs.\"\n",
    "3) Keep the answer to 1-3 sentences.\n",
    "4) Add citations like [source: ...] when available.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=SYSTEM_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever, combine_docs_chain_kwargs={\"prompt\": prompt}, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "254d890b-a30c-46b8-adea-90f705905514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "QUESTION: who is john?\n",
      "ANSWER: I'm not sure from the docs.\n",
      "===========================\n",
      "QUESTION: what if No tracking movement for 7 days?\n",
      "ANSWER: If no tracking movement for 7 days, we open a carrier trace.\n",
      "===========================\n",
      "QUESTION: what is the contact?\n",
      "ANSWER: I'm not sure from the docs.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "test_questions = [\"who is john?\", \"what if No tracking movement for 7 days?\", \"what is the contact?\"]\n",
    "\n",
    "for q in test_questions:\n",
    "    res = chain({\"question\": q, \"chat_history\": chat_history})\n",
    "    answer = res[\"answer\"]\n",
    "    chat_history.append((q, answer))\n",
    "\n",
    "    print(\"===========================\")\n",
    "    print(\"QUESTION:\", q)\n",
    "    print(\"ANSWER:\", res[\"answer\"])\n",
    "    # print(\"SOURCES:\", len(res[\"source_documents\"]))\n",
    "    # for i, d in enumerate(res[\"source_documents\"][:5]):\n",
    "    #     print(\"\\n--- source doc\", i, \"---\", d.metadata.get(\"source\"))\n",
    "    #     print(d.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ce34090-5a37-44a9-b433-688b8541d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We open a carrier trace.\n"
     ]
    }
   ],
   "source": [
    "q = \"what if No tracking movement for 7 days?\"\n",
    "docs = retriever.get_relevant_documents(q)\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    [f\"[source: {d.metadata.get('source','')}] {d.page_content}\" for d in docs]\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(context=context, question=q)\n",
    "# print(final_prompt[:])   # inspect the first ~2000 chars\n",
    "\n",
    "print(llm.invoke(final_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6190a0cb-39fe-4d4a-8a35-5080d9128730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 23:22:06.690 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.691 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.742 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/thanhnguyen/anaconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-12-19 23:22:06.742 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.744 Session state does not function when running a script without `streamlit run`\n",
      "2025-12-19 23:22:06.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.747 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-19 23:22:06.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Demo\", page_icon=\"üí¨\")\n",
    "\n",
    "st.title(\"üìÑ RAG Chat Demo\")\n",
    "st.caption(\"Ask questions based on the indexed documents\")\n",
    "\n",
    "# --- session state ---\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# --- input box ---\n",
    "question = st.text_input(\"Ask a question\")\n",
    "\n",
    "if question:\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        result = chain({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": st.session_state.chat_history\n",
    "        })\n",
    "\n",
    "        answer = result[\"answer\"]\n",
    "        st.session_state.chat_history.append((question, answer))\n",
    "\n",
    "# --- display chat ---\n",
    "for q, a in st.session_state.chat_history[::-1]:\n",
    "    st.markdown(f\"**You:** {q}\")\n",
    "    st.markdown(f\"**Assistant:** {a}\")\n",
    "    st.markdown(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "rag-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
